\section{\label{sec:intro}Introduction}
In todays applications, data is continuously produced in various spots
ranging from network traffic, log file data, monitoring of
manufacturing processes or scientific experiments. The applications
typically emit data in non-terminating data streams at a high rate,
which imposes demanding challenges on the analysis of such streams.

We illustrate this by projects of our Collaborative Research Center
SFB-876. A first example is given by the FACT telescope that is
associated to project C3. This telescope observes cosmic showers by
tracking light that is produced by these showers in the atmosphere
with a camera.  These showers last about 20 nanoseconds and are
recorded with a camera of 1440 pixels at a sampling rate of 2 GHz. As
about 60 of these showers are currently recorded each second, a
5-minute recording interval quickly produces several gigabytes of raw
data.

Other high-volume data is produced in monitoring system behavior, as
performed in project A1. Here, operating systems are monitored by
recording fine grained logs of system calls to catch typical usage of
the system and optimize its resource utilization (e.g. for energy
saving). System calls occur at a high rate and recording produces a
plethora of log entries.

The project B3 focuses on monitoring (distributed) sensors in an
automated manufacturing process. These sensors emit detailed
information about the furnace heat or milling pressure of steel production
and are recorded at fine grained time intervals. Analysis of this data
focuses on supervision and optimization of the production process.

\subsubsection*{From Batches to Streams}
The traditional batch data processing aims at computations on fixed
chunks of data in one or more passes. Such data is usually stored in
files or database systems providing random access to each record. The
results of these computations again form a fixed outcome that can
further be used as input. A simple example is given by the computation
of a prediction model based on some fixed set of training data. After
the determination of a final model, the learning step is finished and
the model is applied to deliver predictions based on the learning
phase. Similar situations arise for the computation of statistics,
creation of histograms, plots and the like. From a machine learning
perspective, this has been the predominant approach of the last years.

Two fundamental aspects have changed in the data we are facing today,
requiring a paradigm shift: The size of data sets has grown to amounts
intractable by existing batch approaches, and the rate at which data
changes demands for short-term reactions to data drifts and updates of
the models.

%\subsubsection*{Processing Masses of Data}
The problem of big data has generally been addressed by massive
parallelism. With the drop of hardware prizes and evolving use of
large cloud setups, computing farms are deployed to handle data at a
large scale. Though parallelism and concepts for cluster computing
have been studied for long, their applicability was mostly limited to
specific use cases.

One of the most influential works to use computing clusters in data
analysis is probably Google's revival of the {\em map-and-reduce}
paradigm \cite{googleMapReduce}. The concept has been around in
functional programming for years and has now been transported to
large-scale cluster systems consisting of thousands of compute
nodes. Apache's open-source {\em Hadoop} implementation of a
map-and-reduce platform nowadays builds a foundation for various
large-scale systems.

With the revival of map-and-reduce, various machine learning algorithms
have been proven to be adjustable to this new (old) way of computing.

\subsection{\label{sec:streamSetting}The Problem of Continuous Data}
Whereas the massive parallelism addresses the batch computation of
large volumes of data, it still requires substantial processing time
to re-compute prediction models, statistics or indexes once data has
changed. Therefore it does not fully reflect the demands for reacting
to short-term drifts of data.

Within this work we will refer to this as the setting of {\em
  continuous data}, i.e. we consider an unbound source $D$ of data
that continuously emits data items $d_i$. In the following, we model
that data stream as a sequence
$$D = \langle d_0,d_1,\ldots,d_i,\ldots \rangle$$
with $i\rightarrow\infty$. The setting to operate on streaming data is
generally given by the following constraints/requirements:
\begin{itemize}
  \defitem{C1} continuously processing {\em single items} or {\em small batches} of data,
  \defitem{C2} using only a {\em single pass} over the data,
  \defitem{C3} using {\em limited resources} (memory, time),
  \defitem{C4} provide {\em anytime services} (models, statistics).
\end{itemize}


To catch up with the reqirements of large scale and continuous data,
online algorithms have recently received a lot of attention. The focus
of these algorithms is to provide approximate results while limiting
the memory and time resources required for computation. 
%The
%constraints for the data stream setting are generally defined by
%allowing only a single pass over the data, and focusing on
%approximation schemes to deal with the inbalance of data volume to
%computing resources. In addition, models computed on streaming data
%are expected to be queriable at any time.


\subsubsection*{Analysis of Continuous Data}
{\bf TODO: \"{U}berleitungssatz?}
Traditional data analysis methods focus on processing fixed size
batches of data and often require the data (or large portions of it)
to be available in main memory. This renders most approaches useless
for continuously analyzing data that arrives in steady streams. Even
procedures like preprocessing or feature extraction can quickly become
challenging for continuous data, especially when only limited
resources with respect to memory or computing power are available.

At any time $t$ we want to provide some model that reflects the
analysis of the items $d_i$ with $i\le t$.  Typical analysis tasks to
compute on $S$ are
\begin{itemize}
\item Given $d_i \in \mathbb{N}$ - finding the top-$k$ most frequent
  values observed until $t$.
\item For $d_i \in \mathbb{N}^p$ - find the item sets $I \subset
  \mathbb{N}^p$ which most frequently occurred in the $d_i$.
\item With $d_i \subset X$, provide a classifier $c:X \rightarrow Y$,
  that best approximates the real distribution of labeled data $X
  \times Y$ (classification).
\item Provide a clustering $C$ for the data item $d_i$ observed so far (clustering).
\item Find indications on when the overall distribution of the $d_i$
  changes within the stream (concept drift detection).
\end{itemize}
Often, these tasks are further refined to models that focus on a
recent sliding window of the last $w$ data items observed, e.g. we are
interested in the top-$k$ elments of the last 5 minutes. 

Algorithms for solving these tasks on static data sets
exists. However, the challenging requirements in the continuous data
setting are the tight limits on the resources available for
computation. This can for example be realtime constraints, such as a
fixed limit on the time available for processing a data item, or a
bound on the memory available for computation.

Various algorithms have been proposed dedicated to computational
problems on data streams. Examples include online quantile computation
\cite{Greenwald/Khanna/2001a,Arasu/Manku/2004a}, distinct counting of
elements, frequent itemset mining
\cite{Charikar02findingfrequent,goethals2007,Cheng06maintainingfrequent},
clustering \cite{sohler2010,Aggarwal:2003} or training of classifiers
on a stream \cite{Domingos/Hulten/2000a}.

Here, we want to provide an abstract framework for putting online
learning algorithms to good use on data streams.

\subsection{Designing Stream Processes}

%\subsection*{Existing Frameworks}
%Various frameworks exist that support either of these two processing
%modes.
Parallel batch processing is addressing the setting of fixed data and
is of limited use if data is non-stationary but continuously produced,
for example in monitoring applications (server log files, sensor
networks).  A framework that provides online analysis is the MOA
library \cite{moa}, which is a Java library closely related to the
WEKA data mining framework \cite{weka}. MOA provides a collection of
online learning algorithms with a focus on evaluation and
benchmarking.

Aiming at processing high-volume data streams two environments have
been proposed by Yahoo! and Twitter. Yahoo!'s {\em S4} \cite{s4io} as
well as Twitter's {\em Storm} \cite{storm} framework do provide online
processing and storage by building on large cluster infrastructures such
as Apache's Zookeeper infrastructure.

The {\em Storm} engine relies on executing a computing graph, called a
{\em topology} in Storm. The nodes (referred to as {\em Bolts}) and the data sources (referred to as {\em Spouts})
in this graph are user written programs defining the data processing and data acquisition
steps. The topology is then provided by the user by implementing a
Java program that creates the desired topology (a {\em topology
  builder}). To start the topology, the custom topology builder
implemented by the user is given to the storm engine, which creating
the bolts and distributing it along the cluster infrastructure.

While the {\em Storm} engine is known to be fast and scalable, it
requires an in-depth knowledge of the user on how to create a topology
that matches a particular task. Looking from the perspective of a {\em
  data analysts}, this does not match the higher-level
rapid-prototyping needs as is adequate for domain experts e.g. in
projects like telescope data analysis mentioned above.


\TODO{More about User/Developer roles, abstraction levels and toolkit
  building blocks.}

In contrast to these frameworks, the \streams library focuses on
defining a simple abstraction layer that allows for the definition of
stream processes by means of only a few basic conceptual elements.
The resulting processes can be then be easily executed by the \streams
runtime or mapped to different runtime environments such as {\em S4}
or {\em Storm}.

\subsubsection*{Our Contributions}
In this work we introduce the \streams library, a small software
framework that provides an abstract modeling of stream processes. The
objective of this framework is to establish a layer of abstraction
that allows for defining stream processes at a high level, while
providing the glue to connect various existing libraries such as MOA
\cite{moa}, WEKA \cite{weka} or the RapidMiner tool.

The set of existing online algorithms provides a valuable collection
of algorithms, ideas and techniques to build upon. Based on these core
elements we seek to design a process environment for implementing
stream processes by combining implementations of existing online
algorithms, online feature extraction methods and other preprocessing
elements.
%or implement and evaluate custom online algorithms 

Moreover it provides a simple programming API to implement and
integrate custom data processors into the designed stream processes.
The level of abstraction of this programming API is intended to
flawlessly integrate into existing runtime environments like {\em
  Storm} or the RapidMiner platform \cite{rapidminerStreams}.

Our proposed framework supports
\begin{enumerate}
\item Modeling of continuous stream processes, following the {\em
    single-pass} paradigm,
\item Anytime access to services that are provided by the modeled
  processes and the online algorithms deployed in the process setup,
  and
\item Processing of large data sets using limited memory resources
\item A simple environment to implement custom stream processors and
  integrate these into the modeling
\item A collection of online algorithms for counting and classification
\item Incorporation of various existing libraries (e.g. MOA
  \cite{moa}) into the modeled process.
\end{enumerate}

The rest of this report is structured as follows: In Section
\ref{sec:abstraction} we derive a set of basic building blocks for the
abstract modeling data stream processes. In Section
\ref{sec:processDesign} we present the XML based definition language
and several addition concepts that allow for designing stream
processes within the framework. Based on this we outline two example
use-cases for processing and analyzing streaming data with the
\streams library. Finally we summarize the ideas behind the \streams
library and give an outlook on future work in Section
\ref{sec:summary}. A comprehensive description of the implementations
and guides for setting up a standalone processing environment provided
by our framework is given in the appendix.
