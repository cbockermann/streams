\section{Introduction}
More and more applications rely on dynamic data that is produced in
realtime and at a high volume. Scientific experiments, network
traffic, sensor networks in manifacturing processes or message
services are examples of such applications. Often the data in these
applications is outdated quickly and reactions need to be applied in
near to realtime. An example is given by Google's news search, which
uses a dynamic index for searching even more recent news articles. In
other scenarios an on-time analysis might save resources as irrelevant
data can quickly be detected and discarded. Analysis in such dynamic
data settings is different to the traditional batch setting that
dominates the field of data analysis.  

Continuous data poses several challenges for data analysts: The data
are often produced at large volume and require continuous processing
to provide up-to-date prediction models or summaries. The models or
statistics incrementally learnt during data processing need to be
accessible at anytime. For processing that data only limited resources
with regard to memory, CPU and I/O is available. 

Recent approaches such as Google's Map/Reduce paradigm address these
challenges by large scale parallelization of batch processes
\cite{googleMapReduce,radoop}. While this scales well with the large
amounts of data at hand, it does not tackle the problem of processing
data {\em continuously}.

%The massive amounts of data continuously produced by scientific
%experiments or in large scale applications demand for alternative
%approaches to traditional random-access and in-memory data analysis.
%As an example, the FACT project\cite{fact} runs a telescope recording
%up to one terabyte of raw data per night. As analysis of this data
%just started, it is not yet clear which parts of the data might be
%useful and which might be subject to being discarded.

To catch up with the reqirements of large scale and continuous data,
online algorithms have recently received a lot of attention. Various
algorithms have been proposed for online quantile computation
\cite{Greenwald/Khanna/2001a,Arasu/Manku/2004a}, frequent itemset
mining
\cite{Charikar02findingfrequent,goethals2007,Cheng06maintainingfrequent},
clustering \cite{sohler2010,Aggarwal:2003} or classification
\cite{Domingos/Hulten/2000a}.


\subsection{Our Contributions}
In this work we introduce the \streams library, a small software
framework that focuses on online processing of data and its adaption
into RapidMiner as the \plugin. The \streams framework provides a
thin abstraction layer to facilitate online data processing whereas
the \plugin\ uses a generic wrapper approach\footnote{RapidMiner
  operators are automatically generated using the \textsf{RapidMiner
    Beans} library, which allows for the implementation of operators
  by following the JavaBeans convention and using simple Java
  annotations.} to build a streaming facade within RapidMiner.

The proposed framwork supports
\begin{enumerate}
\item Modelling of continuous stream processes within RapidMiner,
  following the {\em single-pass} paradigm,
\item Anytime access to services that are provided by the modeled
  processes and the online algorithms deployed in the process setup,
  and
\item Processing of large data sets using limited memory resources.
\item A simple environment to implement custom stream processors and
  integrate these into the modelling
\item Incorporation of various existing libraries (e.g. MOA
  \cite{moa}) into the modeled process
\end{enumerate}

In Section
\ref{sec:relatedWork} we review the problem setting and give an
overview of related work and existing frameworks.
%% introduce some of the main objectives in data
%stream mining and provide some example use cases. 
Based on this we derive some basic building blocks for a modeling data
stream processes (Section \ref{sec:abstraction}). In Section
\ref{sec:streamsLibrary} we present the \streams API which provides
implementations to these building blocks. Finally we summarize the
ideas behind the \streams library and give an outlook on future work.
