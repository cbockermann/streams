\clearpage
\section{\label{sec:machineLearning}Machine Learning with Continuous Data}
As the large volumes of data are merely managable with automatic
processing of that data, they are far away from being inspected
manually. On the other hand gaining insight from that data is the key
problem in various application domains.

Machine learning and data mining has put forth a plethora of
techniques and algorithms for pattern recognition, learning of
prediction model or clustering that all aim at exactly that key
problem: knowledge discovery from data.

For the setting of continuous data, various algorithms have been
proposed which solve basic tasks inherent to the knowledge discovery
process as well as complex methods that allow for training classifiers
or finding clusters on steady streams of data. In this section we will
give an overview of how machine learning algorithms are embedded into
the \streams framework using a simple Naive Bayes classifier as example.
% that have been
%implemented within the \streams framework as well as the integration
%of an existing library, namely the MOA \cite{moa}, which provides a
%rich set of state of the art online learning algorithms. 

We first give an overview of the data representation that is used for
learning from the {\em data items} that represent the basic data format
of {\em streams}. Following that, we outline how learning algorithms are
embedded into the framework of continuous processes. Here we provide an
example for online classification and the computation of statistics.

Based on the embedding of online learning schemes for classification,
we show the integration of the MOA library into the \streams
framework, which allows for directly using the set of existing
classifiers for learning (Section \ref{sec:moa}).

%Following that, we outline the existing online learning implementations
%provided by the {\em streams-analysis} package in Section \ref{sec:streamsAnalysis}.
%A large set of online learning methods is already provided by the {\em MOA}
%library, which is directly integrated into the {\em streams-analysis} package.
%We give details on the integration of {\em MOA} in Section \ref{sec:moa}.

The evaluation of online learning often requires large amounts of data.
In Section \ref{sec:syntheticData} we show how to generate synthetic data
streams for testing online learning algorithms.

%After that overview in Section \ref{sec:onlineLearning}, we discuss
%the problem of the {\em online application} of machine learning models
%which have either been trained online of offline, and then are used to
%make real-life predictions on streaming data. This will be covered in
%Section \ref{sec:onlineApplication}.

%\subsubsection*{Notation used}
%Within this section, we will denote each data item $d_i$ obtained from
%a stream as a tuple $d_i = (d_{i,1},\ldots,d_{i,k}) \subset
%M_1\times\ldots\times M_k$, where each $M_i$ might refer to some
%domain, e.g. $M_i \subseteq \mathbb{R}$ or an arbitrary set. Further
%we will refer to the index $i$ as the index of the item $d_i$ in a
%data stream $D$, i.e.
%\begin{displaymath}
%  D = \langle d_0,d_1,\ldots \rangle.
%\end{displaymath}


%%
%%
%%
\subsection{\label{sec:onlineLearning}Online Learning from Data Streams}
The general definitions of learning tasks in online learning do not
differ from the traditional objectives. Supervised learning such as
classification or regression tasks rely on a source of training data
to build models that can then be applied to new data for prediction.

% ... \baustelle
Learning from unbounded and continuous data imposes demanding challenges to
the designer of machine learning algorithms. Even simple basic
building blocks like the computation of a median or minima/maxima
values that might be required in a learning algorithms tend to become
difficult.

\subsubsection{\label{sec:dataExamples}Learning from Data Items}
Online learning algorithms usually require a data representation
similar to batch learning methods. Typically instances or examples
used for learning are tuples of some real-valued or finite space.

As an example, the task of (binary) classification can be stated as
estimating a function $\hat{f}$ that best approximates a true
(unknown) distribution of instances $(x,y)$ where $x\in M^r$ and $y
\in \{ -1,1 \}$. Usually features are encoded such that $M=\mathbb{R}$
in many application domains.

In the \streams framework we encode each of these tuples as data
items by defining a key $k_i$ for each dimension of $M^p$ and a
special key for the label $y$. By convention, special keys are
prefixed with an {\ttfamily @} character. These special keys are
expected to be ignored as attributes by any learning algorithm.
Figure \ref{fig:exampleItem} shows an instance of learning that
is represented by a data item.

\begin{figure}[h!]
  \centering
$(x,y) \stackrel{\mbox{\scriptsize e.g.}}{=} (0.3,0.57,\ldots,0.413,-1) \ \ \rightarrow\ \  $ {\footnotesize \begin{tabular}{c|c|c|c|c}
{\bf \textsf{Key}} & {\ttfamily x1} & $\cdots$ & {\ttfamily xp} & {\ttfamily @label} \\ \hline
{\bf \textsf{Value}} & 0.3 & $\cdots$ & 0.413 & -1
%{\bf Value} & {\ttfamily x1} & 0.4 \\ 
%$\vdots$ & $\vdots$ \\ 
%{\ttfamily xp} & 0.413 \\
%{\ttfamily y} & -1 \\
\end{tabular}}
  \caption{\label{fig:exampleItem}Data item representation of an instance for learning, key/value table transposed for brevity.}
\end{figure}

As the attributes may hold any {\ttfamily Serializable} values, a
proper pre-processing might be required for applying learning
algorithms, e.g. if these algorithms cannot handle arbitrary data
types. Such preprocessing is for example a String-to-Number conversion
(provided by the {\ttfamily ParseDouble} processor). The \streams
core classes provide a wide number of preprocessing processors.

\subsubsection*{Filtering Attributes}
Sometimes it is desirable to train a classifier only on a subset of
the features/attributes that are contained in the data. The {\ttfamily WithKeys}
processor, allows for the execution of nested processors on filtered
data items. As an example, the XML snippet in Figure \ref{fig:exampleWithKeys}
shows the data preprocessing to apply online learning to the famous Iris
data set with only two of the attributes being selected.

\begin{figure}[h!]
  \centering
  \begin{lstlisting}[language=XML]
    ...
    <process input="iris">
       
        <!-- Rename the "class" attribute to "@label" as by convention
             a learner expects the label in attribute "@label"        -->
        <Rename from="class" to="@label" />

        <!-- select two attributes and the label from the
             data items and apply the inner processors />
        <WithKeys keys="att1,att2,@label">

            <!-- parse the attributes att1 and att2 to Double values -->
            <ParseDouble keys="att1,att2" />

            <!-- feed the data item to a naive bayes classifier for training -->
            <stream.classifier.NaiveBayes id="myNaiveBayes" />
        </WithKeys>

    </process>
  \end{lstlisting}
  \caption{\label{fig:exampleWithKeys}Example XML for training a
    classifier on a subset of attributes.}
\end{figure}


%\subsubsection*{Approximating Distributions}
%As a simple example, the {\em NaiveBayes}\cite{NB} classifier often
%offers a adequate prediction performance in a lot of application
%domains. Starting with the independence assumption of attributes, it
%maximizes computes the class probabilities given the observed
%attributes of a set of training instances. The base rule of bayes is
%given in equation (\ref{eqn:naiveBayes}):
%\begin{eqnarray}
%  P(c | f_1,\ldots,f_n ) = \frac{P(c)\cdot P(f_1,\ldots,f_n|c)}{P(f_1,\ldots,f_n)}.\label{eqn:naiveBayes}
%\end{eqnarray}

%Assuming a fixed set $C = \{c_1,\ldots,c_l\}$ of observable classes, we
%can easily approximate $P(c)$ by counting the occurences for each
%$c_i$ in the observed stream. For pure numerical attributes $f_i$,
%generally a gaussian normal distribution is assumed, such that the
%factors $P(f_1,\ldots,f_n|c)$ and $P(f_1,\ldots,f_n)$ can be derived
%by estimating the mean and average for each attribute $f_i$.

%The setting gets a bit more complicated if the $f_i$ are nominal
%values, such as variable strings from an unbounded domain such as
%URLs, i.e. $M_i \cong \mathbb{N}$. In this case we cannot simply
%derive a probability for each instance of an attribute as this would
%require counting an unbounded set of strings, which clearly violates
%the stream processing contraints mentioned in Section
%\ref{sec:streamSetting}.

%%
%% How are classifiers embedded into the streams framework?
%% How can they be used?
%%
%\subsubsection{Embedding Classifiers in \streams}


%%
%% Which classifiers/clusterers/etc. are available?
%%
%\subsubsection{Available Online Learning Algorithms}

%\subsubsection*{Online Statistics (Counting, Quantiles)}
%
%\subsubsection*{Online Classifier}

\include{streams-analysis-core}

\include{streams-analysis-moa}

\include{streams-analysis-synthetic}

%\include{online-model-application}